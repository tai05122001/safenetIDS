import pandas as pd
import sys
import os

# Add current directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from network_data_producer import NetworkDataProducer
import time
import random
from datetime import datetime
import logging
import os
import sys
from kafka import KafkaConsumer, KafkaAdminClient
from kafka.admin import NewPartitions
from kafka.structs import TopicPartition

# T·∫°o th∆∞ m·ª•c logs n·∫øu ch∆∞a t·ªìn t·∫°i (ph·∫£i l√†m tr∆∞·ªõc khi kh·ªüi t·∫°o FileHandler)
os.makedirs('services/logs', exist_ok=True)

# C·ªë g·∫Øng set UTF-8 encoding cho console tr√™n Windows
if sys.platform == 'win32':
    try:
        # Th·ª≠ set console code page sang UTF-8
        import subprocess
        subprocess.run(['chcp', '65001'], shell=True, capture_output=True, check=False)
        # Set stdout/stderr encoding n·∫øu c√≥ th·ªÉ
        if hasattr(sys.stdout, 'reconfigure'):
            sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        if hasattr(sys.stderr, 'reconfigure'):
            sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass  # N·∫øu kh√¥ng ƒë∆∞·ª£c th√¨ b·ªè qua

# Custom StreamHandler v·ªõi error handling cho Windows console
class SafeStreamHandler(logging.StreamHandler):
    """StreamHandler v·ªõi error handling cho encoding issues tr√™n Windows"""
    def emit(self, record):
        try:
            msg = self.format(record)
            stream = self.stream
            
            # Th·ª≠ ghi tr·ª±c ti·∫øp
            try:
                stream.write(msg + self.terminator)
                self.flush()
            except (UnicodeEncodeError, UnicodeDecodeError):
                # N·∫øu l·ªói encoding, chuy·ªÉn ƒë·ªïi message sang ASCII-safe
                # b·∫±ng c√°ch thay th·∫ø c√°c k√Ω t·ª± kh√¥ng encode ƒë∆∞·ª£c b·∫±ng '?'
                safe_msg = msg.encode('ascii', errors='replace').decode('ascii', errors='replace')
                try:
                    stream.write(safe_msg + self.terminator)
                    self.flush()
                except Exception:
                    # N·∫øu v·∫´n l·ªói, b·ªè qua
                    self.handleError(record)
        except Exception:
            self.handleError(record)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('services/logs/simulate_attack_cnn.log', encoding='utf-8'),
        SafeStreamHandler()
    ]
)
logger = logging.getLogger('SimulateAttackCNN')

def reset_consumer_group_offset(kafka_bootstrap_servers='localhost:9092', 
                                topic='raw_data_event_cnn',
                                group_id='safenet-cnn-preprocessing-group'):
    """
    Reset offset c·ªßa consumer group v·ªÅ cu·ªëi topic ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ ƒë·ªçc d·ªØ li·ªáu m·ªõi
    
    Args:
        kafka_bootstrap_servers: Kafka bootstrap servers
        topic: Topic name
        group_id: Consumer group ID c·∫ßn reset
    """
    try:
        logger.info(f"Resetting offset for consumer group '{group_id}' on topic '{topic}'...")
        
        # T·∫°o consumer t·∫°m v·ªõi group_id kh√°c ƒë·ªÉ l·∫•y partition info
        # ƒêi·ªÅu n√†y tr√°nh conflict v·ªõi consumer ƒëang ch·∫°y
        temp_consumer = KafkaConsumer(
            topic,
            bootstrap_servers=kafka_bootstrap_servers,
            group_id=f'{group_id}-temp-reset-{int(time.time())}',  # Group ID t·∫°m th·ªùi unique
            enable_auto_commit=False,
            auto_offset_reset='latest',
            consumer_timeout_ms=5000  # Timeout sau 5 gi√¢y
        )
        
        # Poll ƒë·ªÉ trigger partition assignment
        temp_consumer.poll(timeout_ms=2000)
        partitions = temp_consumer.assignment()
        
        if not partitions:
            logger.warning(f"Could not get partitions for topic '{topic}'")
            temp_consumer.close()
            return
        
        # L·∫•y end offsets cho t·∫•t c·∫£ partitions
        end_offsets = temp_consumer.end_offsets(partitions)
        logger.info(f"Found {len(partitions)} partitions, end offsets: {end_offsets}")
        
        temp_consumer.close()
        
        # B√¢y gi·ªù t·∫°o consumer v·ªõi group_id th·ª±c ƒë·ªÉ reset offset
        # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ consumer c≈© c√≥ th·ªÉ rejoin n·∫øu c·∫ßn
        time.sleep(1)
        
        reset_consumer = KafkaConsumer(
            topic,
            bootstrap_servers=kafka_bootstrap_servers,
            group_id=group_id,
            enable_auto_commit=False,
            auto_offset_reset='latest',
            consumer_timeout_ms=10000
        )
        
        # Poll v√† ƒë·ª£i partition assignment (c√≥ th·ªÉ m·∫•t th·ªùi gian n·∫øu consumer c≈© ƒëang ch·∫°y)
        logger.info("Waiting for partition assignment (consumer may need to rejoin group)...")
        assigned = None
        for attempt in range(15):  # Th·ª≠ t·ªëi ƒëa 15 l·∫ßn (15 gi√¢y)
            reset_consumer.poll(timeout_ms=1000)
            assigned = reset_consumer.assignment()
            if assigned:
                logger.info(f"Partitions assigned: {assigned}")
                break
            if attempt < 14:  # Kh√¥ng sleep ·ªü l·∫ßn cu·ªëi
                time.sleep(0.5)
        
        if not assigned:
            logger.warning("Could not get partition assignment after waiting, cannot reset offset")
            reset_consumer.close()
            return
        
        # ƒê·∫£m b·∫£o partition ƒë√£ ƒë∆∞·ª£c assign tr∆∞·ªõc khi seek
        # Ch·ªâ seek partition c√≥ trong assignment v√† c√≥ trong end_offsets
        seeked_count = 0
        for partition in assigned:
            # Ki·ªÉm tra partition c√≥ trong assignment v√† c√≥ trong end_offsets
            if partition in reset_consumer.assignment() and partition in end_offsets:
                try:
                    reset_consumer.seek(partition, end_offsets[partition])
                    logger.info(f"Seeked partition {partition.partition} to offset {end_offsets[partition]}")
                    seeked_count += 1
                except AssertionError as e:
                    logger.warning(f"Partition {partition.partition} not assigned, skipping: {e}")
                except Exception as e:
                    logger.warning(f"Failed to seek partition {partition.partition}: {e}")
            else:
                logger.warning(f"Partition {partition.partition} not in assignment or end_offsets, skipping")
        
        if seeked_count > 0:
            # Commit offset
            try:
                reset_consumer.commit()
                logger.info(f"Successfully reset and committed offset for consumer group '{group_id}' ({seeked_count} partitions)")
            except Exception as e:
                logger.warning(f"Failed to commit offset: {e}")
        else:
            logger.warning("No partitions were seeked, cannot reset offset")
        
        reset_consumer.close()
        logger.info("Offset reset completed")
        
    except Exception as e:
        logger.error(f"Failed to reset consumer group offset: {e}")
        import traceback
        logger.error(traceback.format_exc())
        logger.warning("Continuing without offset reset - consumer may read old messages")

def simulate_attack_from_pkl(num_samples=5, pkl_file='dataset.pkl', kafka_bootstrap_servers='localhost:9092', topic='raw_data_event_cnn', force_reset=True):
    """
    T·∫£i d·ªØ li·ªáu t·ª´ dataset.pkl, ch·ªçn c√°c m·∫´u ƒë·∫°i di·ªán cho t·∫•t c·∫£ lo·∫°i t·∫•n c√¥ng
    ƒë·ªÉ ƒë·∫£m b·∫£o c·∫£ Level 1 v√† Level 2 model ƒë·ªÅu c√≥ th·ªÉ ho·∫°t ƒë·ªông.

    **M·∫∂C ƒê·ªäNH KHI START (5 samples m·ªói lo·∫°i, oversample n·∫øu c·∫ßn):**
    - BENIGN: 5 samples (kh√¥ng t·∫•n c√¥ng)
    - DoS attacks: 20 samples (5 samples m·ªói lo·∫°i DoS: hulk, goldeneye, slowloris, slowhttptest)
    - DDoS: 5 samples
    - PortScan: 5 samples

    T·ªïng c·ªông: 5 + 20 + 5 + 5 = 35 samples

    N·∫øu dataset kh√¥ng c√≥ ƒë·ªß m·∫´u cho m·ªôt lo·∫°i n√†o ƒë√≥, s·∫Ω oversample (l·∫•y m·∫´u c√≥ l·∫∑p l·∫°i)
    ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ ƒë√∫ng s·ªë l∆∞·ª£ng m·∫´u mong mu·ªën.
    """
    try:
        df = pd.read_pickle(pkl_file)
        logger.info(f"Loaded {len(df)} records from {pkl_file}")

        # T√¨m c·ªôt label (c√≥ th·ªÉ c√≥ nhi·ªÅu t√™n kh√°c nhau)
        # Th·ª≠ c√°c t√™n ph·ªï bi·∫øn: Label, label, label_encoded, label_group, etc.
        label_col = None
        possible_label_cols = ['Label', 'label', 'label_encoded', 'label_group', 
                              'Label_encoded', 'Label_group', 'target', 'Target']
        
        for col in possible_label_cols:
            if col in df.columns:
                label_col = col
                logger.info(f"Found label column: '{col}'")
                break
        
        # N·∫øu kh√¥ng t√¨m th·∫•y, log t·∫•t c·∫£ c√°c c·ªôt ƒë·ªÉ debug
        if label_col is None:
            logger.warning("Kh√¥ng t√¨m th·∫•y c·ªôt label trong dataset!")
            logger.info(f"Dataset c√≥ {len(df.columns)} c·ªôt:")
            logger.info(f"C√°c c·ªôt: {', '.join(df.columns.tolist()[:20])}")  # Hi·ªÉn th·ªã 20 c·ªôt ƒë·∫ßu
            if len(df.columns) > 20:
                logger.info(f"... v√† {len(df.columns) - 20} c·ªôt kh√°c")
            
            # Th·ª≠ t√¨m c·ªôt c√≥ ch·ª©a 'label' trong t√™n (case-insensitive)
            label_like_cols = [col for col in df.columns if 'label' in str(col).lower()]
            if label_like_cols:
                logger.info(f"T√¨m th·∫•y c√°c c·ªôt c√≥ ch·ª©a 'label': {', '.join(label_like_cols)}")
                # S·ª≠ d·ª•ng c·ªôt ƒë·∫ßu ti√™n t√¨m th·∫•y
                label_col = label_like_cols[0]
                logger.info(f"S·ª≠ d·ª•ng c·ªôt '{label_col}' l√†m label column")
        
        # ƒê·ªãnh nghƒ©a c√°c lo·∫°i t·∫•n c√¥ng c·∫ßn thi·∫øt (d√πng cho c·∫£ khi c√≥ v√† kh√¥ng c√≥ label_col)
        # DoS attacks (cho Level 2 dos model) - chia ƒë·ªÅu c√°c lo·∫°i
        # ƒê√£ b·ªè Heartbleed kh·ªèi dataset
        dos_subtypes = {
            'hulk': ['DoS Hulk', 'dos hulk', 'DoS attack'],
            'goldeneye': ['DoS GoldenEye', 'dos goldeneye'],
            'slowloris': ['DoS slowloris', 'dos slowloris'],
            'slowhttptest': ['DoS Slowhttptest', 'dos slowhttptest']
        }
        # T·∫•t c·∫£ DoS labels (ƒë·ªÉ t√¨m trong dataset)
        dos_labels = []
        for subtype_labels in dos_subtypes.values():
            dos_labels.extend(subtype_labels)
        
        # Benign
        benign_labels = ['BENIGN', 'Benign', 'benign']
        
        # DDoS
        ddos_labels = ['DDoS', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP', 
                      'ddos', 'ddos attack-hoic', 'ddos attack-loic-udp']
        
        # PortScan
        portscan_labels = ['PortScan', 'portscan', 'Port Scan', 'port scan']
        
        if label_col is None:
            logger.warning("Kh√¥ng t√¨m th·∫•y c·ªôt label, ch·ªçn ng·∫´u nhi√™n")
            if len(df) < num_samples:
                samples_df = df
            else:
                random_indices = random.sample(range(len(df)), num_samples)
                samples_df = df.iloc[random_indices]
        else:
            selected_samples = []
            samples_per_type = 5  # M·ªói lo·∫°i l·∫•y ƒë√∫ng 5 samples (oversample n·∫øu c·∫ßn)
            
            # 1. Ch·ªçn BENIGN (5 samples) - Oversample n·∫øu c·∫ßn
            benign_df = df[df[label_col].isin(benign_labels)]
            if len(benign_df) > 0:
                # Oversample n·∫øu c·∫ßn ƒë·ªÉ c√≥ ƒë√∫ng 5 m·∫´u
                benign_samples = benign_df.sample(n=samples_per_type, replace=True, random_state=42)
                selected_samples.append(benign_samples)
                logger.info(f"Selected {len(benign_samples)} BENIGN samples (oversampled if needed)")
            else:
                logger.warning("Kh√¥ng t√¨m th·∫•y BENIGN samples trong dataset!")

            # 2. Ch·ªçn DoS attacks - 5 samples M·ªñI lo·∫°i DoS (oversample n·∫øu c·∫ßn)
            for subtype_name, subtype_labels in dos_subtypes.items():
                subtype_df = df[df[label_col].isin(subtype_labels)]
                if len(subtype_df) > 0:
                    # Oversample n·∫øu c·∫ßn ƒë·ªÉ c√≥ ƒë√∫ng 5 m·∫´u cho m·ªói subtype
                    subtype_samples = subtype_df.sample(n=samples_per_type, replace=True, random_state=42)
                    selected_samples.append(subtype_samples)
                    logger.info(f"Selected {len(subtype_samples)} DoS {subtype_name} samples (oversampled if needed)")
                else:
                    logger.warning(f"Kh√¥ng t√¨m th·∫•y DoS {subtype_name} samples trong dataset!")

            # 3. Ch·ªçn DDoS (5 samples) - Oversample n·∫øu c·∫ßn
            ddos_df = df[df[label_col].isin(ddos_labels)]
            if len(ddos_df) > 0:
                # Oversample n·∫øu c·∫ßn ƒë·ªÉ c√≥ ƒë√∫ng 5 m·∫´u
                ddos_samples = ddos_df.sample(n=samples_per_type, replace=True, random_state=42)
                selected_samples.append(ddos_samples)
                logger.info(f"Selected {len(ddos_samples)} DDoS samples (oversampled if needed)")
            else:
                logger.warning("Kh√¥ng t√¨m th·∫•y DDoS samples trong dataset!")

            # 4. Ch·ªçn PortScan (5 samples) - Oversample n·∫øu c·∫ßn
            portscan_df = df[df[label_col].isin(portscan_labels)]
            if len(portscan_df) > 0:
                # Oversample n·∫øu c·∫ßn ƒë·ªÉ c√≥ ƒë√∫ng 5 m·∫´u
                portscan_samples = portscan_df.sample(n=samples_per_type, replace=True, random_state=42)
                selected_samples.append(portscan_samples)
                logger.info(f"Selected {len(portscan_samples)} PortScan samples (oversampled if needed)")
            else:
                logger.warning("Kh√¥ng t√¨m th·∫•y PortScan samples trong dataset!")
            
            # G·ªôp t·∫•t c·∫£ samples
            if selected_samples:
                samples_df = pd.concat(selected_samples, ignore_index=True)
                # X√°o tr·ªôn ƒë·ªÉ kh√¥ng g·ª≠i theo th·ª© t·ª±
                samples_df = samples_df.sample(frac=1, random_state=42).reset_index(drop=True)
            else:
                logger.warning("Kh√¥ng ch·ªçn ƒë∆∞·ª£c m·∫´u n√†o, s·ª≠ d·ª•ng random selection")
                if len(df) < num_samples:
                    samples_df = df
                else:
                    random_indices = random.sample(range(len(df)), num_samples)
                    samples_df = df.iloc[random_indices]
        
        samples = samples_df.to_dict('records')
        logger.info(f"Selected {len(samples)} samples for attack simulation.")
        
        # Log ph√¢n b·ªë c√°c lo·∫°i t·∫•n c√¥ng ƒë∆∞·ª£c ch·ªçn v√† m·∫´u n√†o ch∆∞a c√≥
        if label_col and label_col in samples_df.columns:
            label_counts = samples_df[label_col].value_counts()
            logger.info("=" * 60)
            logger.info("SUMMARY: Attack types distribution in selected samples:")
            logger.info("=" * 60)
            for label, count in label_counts.items():
                logger.info(f"  [OK] {label}: {count} samples")
            
            # Ki·ªÉm tra c√°c lo·∫°i t·∫•n c√¥ng c·∫ßn thi·∫øt ƒë√£ c√≥ ch∆∞a
            logger.info("")
            logger.info("=" * 60)
            logger.info("CHECK: Required attack types availability:")
            logger.info("=" * 60)
            
            # Ki·ªÉm tra BENIGN
            benign_found = any(label in benign_labels for label in label_counts.index)
            if benign_found:
                benign_count = sum(count for label, count in label_counts.items() if label in benign_labels)
                logger.info(f"  [OK] BENIGN: {benign_count} samples")
            else:
                logger.warning("  [X] BENIGN: NOT FOUND in selected samples")
            
            # Ki·ªÉm tra DoS attacks
            dos_found = any(label in dos_labels for label in label_counts.index)
            if dos_found:
                dos_count = sum(count for label, count in label_counts.items() if label in dos_labels)
                dos_types = [label for label in label_counts.index if label in dos_labels]
                logger.info(f"  [OK] DoS attacks: {dos_count} samples")
                logger.info(f"    Types found: {', '.join(dos_types)}")
            else:
                logger.warning("  [X] DoS attacks: NOT FOUND in selected samples")
            
            # Ki·ªÉm tra DDoS
            ddos_found = any(label in ddos_labels for label in label_counts.index)
            if ddos_found:
                ddos_count = sum(count for label, count in label_counts.items() if label in ddos_labels)
                ddos_types = [label for label in label_counts.index if label in ddos_labels]
                logger.info(f"  [OK] DDoS: {ddos_count} samples")
                logger.info(f"    Types found: {', '.join(ddos_types)}")
            else:
                logger.warning("  [X] DDoS: NOT FOUND in selected samples")
            
            # Ki·ªÉm tra PortScan
            portscan_found = any(label in portscan_labels for label in label_counts.index)
            if portscan_found:
                portscan_count = sum(count for label, count in label_counts.items() if label in portscan_labels)
                portscan_types = [label for label in label_counts.index if label in portscan_labels]
                logger.info(f"  [OK] PortScan: {portscan_count} samples")
                logger.info(f"    Types found: {', '.join(portscan_types)}")
            else:
                logger.warning("  [X] PortScan: NOT FOUND in selected samples")
            
            logger.info("=" * 60)
            
            # Summary v·ªÅ prediction flow
            logger.info("")
            logger.info("=" * 60)
            logger.info("PREDICTION SUMMARY: Expected prediction flow (CNN):")
            logger.info("=" * 60)
            
            # Mapping t·ª´ label sang label_group (theo logic trong preprocess_dataset.py)
            label_to_group = {}
            for label in label_counts.index:
                label_lower = str(label).lower().strip()
                if label_lower in ['benign']:
                    label_to_group[label] = 'benign'
                elif any(dos in label_lower for dos in ['dos hulk', 'dos goldeneye', 'dos slowloris', 'dos slowhttptest', 'dos attack', 'heartbleed']):
                    label_to_group[label] = 'dos'
                elif label_lower in ['ddos', 'ddos attack-hoic', 'ddos attack-loic-udp']:
                    label_to_group[label] = 'ddos'
                elif label_lower in ['portscan']:
                    label_to_group[label] = 'portscan'
                # Bot, Infiltration, Heartbleed ƒë√£ ƒë∆∞·ª£c b·ªè kh·ªèi dataset
                # Kh√¥ng c√≤n rare_attack group
                else:
                    label_to_group[label] = 'other'
            
            # Nh√≥m theo label_group
            group_counts = {}
            for label, count in label_counts.items():
                group = label_to_group.get(label, 'other')
                if group not in group_counts:
                    group_counts[group] = {}
                group_counts[group][label] = count
            
            # Log prediction flow
            for group in ['benign', 'dos', 'ddos', 'portscan', 'other']:
                if group in group_counts:
                    total = sum(group_counts[group].values())
                    logger.info(f"")
                    logger.info(f"  Level 1 Prediction: '{group}' ({total} samples)")
                    logger.info(f"    ‚Üí Level 2 Prediction: {'YES' if group == 'dos' else 'NO (not routed to Level 2)'}")
                    if group == 'dos':
                        logger.info(f"    ‚Üí Level 2 will classify into:")
                        for label, count in group_counts[group].items():
                            logger.info(f"        - {label}: {count} samples")
                    else:
                        logger.info(f"    ‚Üí Samples:")
                        for label, count in group_counts[group].items():
                            logger.info(f"        - {label}: {count} samples")
            
            logger.info("=" * 60)
            
            # Log t·∫•t c·∫£ c√°c lo·∫°i c√≥ trong dataset (ƒë·ªÉ tham kh·∫£o)
            if label_col in df.columns:
                all_labels = df[label_col].value_counts()
                logger.info("")
                logger.info("=" * 60)
                logger.info("REFERENCE: All attack types available in dataset:")
                logger.info("=" * 60)
                for label, count in all_labels.items():
                    status = "[OK]" if label in label_counts.index else "[  ]"
                    logger.info(f"  {status} {label}: {count} total samples")
                logger.info("=" * 60)

        # RESET OFFSET TR∆Ø·ªöC KHI G·ª¨I D·ªÆ LI·ªÜU M·ªöI
        # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o cnn_data_preprocessing_service ch·ªâ ƒë·ªçc d·ªØ li·ªáu m·ªõi t·ª´ l·∫ßn ch·∫°y n√†y,
        # kh√¥ng ƒë·ªçc message c≈© t·ª´ l·∫ßn ch·∫°y tr∆∞·ªõc
        reset_consumer_group_offset(
            kafka_bootstrap_servers=kafka_bootstrap_servers,
            topic=topic,
            group_id='safenet-cnn-preprocessing-group'
        )

        # ƒê·ª£i l√¢u h∆°n ƒë·ªÉ ƒë·∫£m b·∫£o offset ƒë√£ ƒë∆∞·ª£c commit v√† consumer ƒë√£ rejoin group
        logger.info("Waiting for offset commit and consumer rejoin (5 seconds)...")
        time.sleep(5)

        producer = NetworkDataProducer(kafka_bootstrap_servers=kafka_bootstrap_servers, topic=topic)

        for i, sample in enumerate(samples):
            # C·∫≠p nh·∫≠t timestamp ƒë·ªÉ ph·∫£n √°nh th·ªùi ƒëi·ªÉm g·ª≠i hi·ªán t·∫°i
            sample['timestamp'] = datetime.now().isoformat()

            # ƒê·∫£m b·∫£o c√≥ ID v√† Label ƒë·ªÉ ƒë·ªëi chi·∫øu
            sample_id = sample.get('id', f"sim_{i}")
            sample_label = sample.get('Label', sample.get('label', 'Unknown'))

            # G·ª≠i d·ªØ li·ªáu
            producer.send_network_data(sample)
            logger.info(f"Sent sample {i+1}/{len(samples)} to Kafka. ID: {sample_id}, LABEL: {sample_label}")
            time.sleep(0.2) # D·ª´ng 0.2 gi√¢y gi·ªØa c√°c l·∫ßn g·ª≠i ƒë·ªÉ ƒë·∫£m b·∫£o consumer k·ªãp x·ª≠ l√Ω

        producer.stop()
        logger.info(f"‚úÖ Attack simulation completed successfully! Sent {len(samples)} samples to Kafka topic '{topic}'")
        logger.info(f"üìä Expected distribution: BENIGN:5, DoS:20, DDoS:5, PortScan:5 = Total:35 samples")
        logger.info(f"‚è±Ô∏è  CNN services should now process these samples. Check CNN preprocessing logs for results.")

    except FileNotFoundError:
        logger.error(f"Error: {pkl_file} not found. Please ensure the dataset file exists.")
        logger.info("Note: Default file is 'dataset_clean_cnn.pkl' in the project root directory.")
    except Exception as e:
        logger.error(f"An error occurred during attack simulation: {e}")

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Simulate Attack for IDS Testing (CNN)')
    parser.add_argument('--num-samples', type=int, default=5, help='Number of samples to simulate per attack type (and benign)')
    parser.add_argument('--pkl-file', default='dataset.pkl', help='PKL file to load data from')
    parser.add_argument('--kafka-servers', default='localhost:9092', help='Kafka bootstrap servers')
    parser.add_argument('--topic', default='raw_data_event_cnn', help='Kafka topic to send data to')
    parser.add_argument('--force-reset', action='store_true', default=True, help='Force reset consumer offset')

    args = parser.parse_args()

    # Run simulation
    simulate_attack_from_pkl(
        num_samples=args.num_samples,
        pkl_file=args.pkl_file,
        kafka_bootstrap_servers=args.kafka_servers,
        topic=args.topic,
        force_reset=args.force_reset
    )
