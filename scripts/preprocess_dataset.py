"""
Script ti·ªÅn x·ª≠ l√Ω dataset: ƒë·ªçc d·ªØ li·ªáu ƒë√£ load (pickle ho·∫∑c CSV), l√†m s·∫°ch v√† l∆∞u l·∫°i pickle.

V√≠ d·ª• ch·∫°y:
# Cho Random Forest (m·∫∑c ƒë·ªãnh):
python scripts/preprocess_dataset.py --source dataset.pkl --output dataset_clean.pkl

# Cho CNN+LSTM:
python scripts/preprocess_dataset.py --source dataset.pkl --output dataset_clean.pkl --model-type cnn_lstm

# Ho·∫∑c ch·ªâ ƒë·ªãnh r√µ cho Random Forest:
python scripts/preprocess_dataset.py --source dataset.pkl --output dataset_clean.pkl --model-type random_forest
"""

from __future__ import annotations
# Cho ph√©p d√πng c√∫ ph√°p type annotation m·ªõi m√† v·∫´n ch·∫°y tr√™n phi√™n b·∫£n Python c≈© h∆°n.

import argparse  # ƒê·ªçc tham s·ªë t·ª´ d√≤ng l·ªánh.
import json  # Xu·∫•t metadata ti·ªÅn x·ª≠ l√Ω ƒë·ªÉ t√°i s·ª≠ d·ª•ng ·ªü c√°c b∆∞·ªõc sau.
import re  # X·ª≠ l√Ω chu·ªói v·ªõi bi·ªÉu th·ª©c ch√≠nh quy.
from pathlib import Path  # L√†m vi·ªác v·ªõi ƒë∆∞·ªùng d·∫´n d·∫°ng object.
from typing import Iterable  # Type hint cho c√°c tham s·ªë d·∫°ng l·∫∑p.

import numpy as np  # H·ªó tr·ª£ thao t√°c s·ªë h·ªçc, v√≠ d·ª• ph√°t hi·ªán gi√° tr·ªã v√¥ h·∫°n.
import pandas as pd  # Th∆∞ vi·ªán x·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng b·∫£ng.

BASE_DIR = Path(__file__).resolve().parent.parent  # Th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n.


def parse_args() -> argparse.Namespace:
    """ƒê·ªçc c·∫•u h√¨nh d√≤ng l·ªánh cho b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω."""
    parser = argparse.ArgumentParser(
        description=(
            "Ti·ªÅn x·ª≠ l√Ω dataset (chu·∫©n h√≥a t√™n c·ªôt, x·ª≠ l√Ω thi·∫øu, m√£ h√≥a nh√£n, l√†m gi√†u ƒë·∫∑c tr∆∞ng) "
            "v√† l∆∞u pickle."
        )
    )
    parser.add_argument(
        "--source",
        type=Path,
        default=Path("dataset.pkl"),
        help="ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu ƒë·∫ßu v√†o (pickle ho·∫∑c CSV). M·∫∑c ƒë·ªãnh: dataset.pkl.",
    )
    parser.add_argument(
        "--fallback-csv",
        type=Path,
        default=Path("dataset.csv"),
        help="CSV d√πng ƒë·ªÉ fallback n·∫øu pickle ch∆∞a t·ªìn t·∫°i (m·∫∑c ƒë·ªãnh: dataset.csv).",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("dataset_clean.pkl"),
        help="ƒê∆∞·ªùng d·∫´n l∆∞u d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω d·∫°ng pickle (m·∫∑c ƒë·ªãnh: dataset_clean.pkl).",
    )
    parser.add_argument(
        "--output-csv",
        type=Path,
        default=None,
        help="(Tu·ª≥ ch·ªçn) L∆∞u th√™m b·∫£n CSV s·∫°ch.",
    )
    parser.add_argument(
        "--label-column",
        type=str,
        default="Label",
        help="T√™n c·ªôt nh√£n (ph√¢n bi·ªát hoa th∆∞·ªùng, m·∫∑c ƒë·ªãnh: Label).",
    )
    parser.add_argument(
        "--drop-duplicates",
        action="store_true",
        help="Lo·∫°i b·ªè d√≤ng tr√πng l·∫∑p sau khi l√†m s·∫°ch.",
    )
    parser.add_argument(
        "--min-non-null-ratio",
        type=float,
        default=0.5,
        help="Ng∆∞·ª°ng t·ªëi thi·ªÉu (0-1) t·ª∑ l·ªá gi√° tr·ªã kh√¥ng null ƒë·ªÉ gi·ªØ c·ªôt (m·∫∑c ƒë·ªãnh 0.5).",
    )
    parser.add_argument(
        "--drop-constant-columns",
        action="store_true",
        help="Lo·∫°i b·ªè c√°c c·ªôt ch·ªâ c√≥ m·ªôt gi√° tr·ªã duy nh·∫•t.",
    )
    parser.add_argument(
        "--outlier-method",
        choices=("none", "iqr_clip"),
        default="none",
        help="Ph∆∞∆°ng ph√°p x·ª≠ l√Ω ngo·∫°i l·ªá: none (m·∫∑c ƒë·ªãnh) ho·∫∑c iqr_clip (clip theo IQR).",
    )
    parser.add_argument(
        "--iqr-factor",
        type=float,
        default=1.5,
        help="H·ªá s·ªë nh√¢n IQR ƒë·ªÉ clip ngo·∫°i l·ªá (m·∫∑c ƒë·ªãnh 1.5).",
    )
    parser.add_argument(
        "--scale-method",
        choices=("none", "standard", "minmax"),
        default="none",
        help=(
            "Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë: none (m·∫∑c ƒë·ªãnh), standard (z-score) ho·∫∑c minmax (0-1). "
            "‚ö†Ô∏è C·∫¢NH B√ÅO: N√™n ƒë·ªÉ 'none' v√¨ model training pipeline ƒë√£ c√≥ StandardScaler. "
            "N·∫øu scale ·ªü ƒë√¢y s·∫Ω b·ªã double scaling ‚Üí k·∫øt qu·∫£ prediction sai!"
        ),
    )
    parser.add_argument(
        "--one-hot",
        action="store_true",
        help="B·∫≠t one-hot encoding cho c√°c c·ªôt ph√¢n lo·∫°i (ngo·∫°i tr·ª´ c·ªôt nh√£n).",
    )
    parser.add_argument(
        "--summary",
        action="store_true",
        help="In th·ªëng k√™ t·ªïng quan sau ti·ªÅn x·ª≠ l√Ω.",
    )
    parser.add_argument(
        "--create-label-group",
        action="store_true",
        help="Sinh th√™m c·ªôt label_group (gom nh√≥m nh√£n ch√≠nh).",
    )
    parser.add_argument(
        "--label-group-column",
        type=str,
        default="label_group",
        help="T√™n c·ªôt ch·ª©a nh√£n nh√≥m (m·∫∑c ƒë·ªãnh: label_group).",
    )
    parser.set_defaults(create_label_group=True)
    parser.add_argument(  # Tham s·ªë l·ª±a ch·ªçn ph∆∞∆°ng ph√°p c√¢n b·∫±ng.
        "--balance-method",
        choices=("none", "oversample", "undersample"),  # C√°c ph∆∞∆°ng √°n h·ªó tr·ª£.
        default="none",  # M·∫∑c ƒë·ªãnh kh√¥ng c√¢n b·∫±ng.
        help="C√¢n b·∫±ng d·ªØ li·ªáu theo nh√£n: none (gi·ªØ nguy√™n), oversample (nh√¢n b·∫£n l·ªõp hi·∫øm), undersample (c·∫Øt b·ªõt l·ªõp l·ªõn).",
    )
    parser.add_argument(  # Seed gi√∫p t√°i l·∫≠p k·∫øt qu·∫£ c√¢n b·∫±ng.
        "--balance-random-state",
        type=int,
        default=42,
        help="Seed ng·∫´u nhi√™n cho b∆∞·ªõc c√¢n b·∫±ng d·ªØ li·ªáu (m·∫∑c ƒë·ªãnh: 42).",
    )
    parser.add_argument(
        "--metadata-output",
        type=Path,
        default=None,
        help="(Tu·ª≥ ch·ªçn) L∆∞u metadata ti·ªÅn x·ª≠ l√Ω (JSON).",
    )
    parser.add_argument(
        "--model-type",
        choices=("random_forest", "cnn_lstm", "both"),
        default="both",
        help="Lo·∫°i model s·ª≠ d·ª•ng: random_forest, cnn_lstm, ho·∫∑c both (ch·∫°y c·∫£ hai). ·∫¢nh h∆∞·ªüng ƒë·∫øn vi·ªác scale v√† encoding.",
    )
    return parser.parse_args()


def normalize_column(name: str) -> str:
    """Chu·∫©n h√≥a t√™n c·ªôt v·ªÅ d·∫°ng snake_case ch·ªØ th∆∞·ªùng, b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát."""
    cleaned = name.strip()  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ·ªü hai ƒë·∫ßu.
    cleaned = re.sub(r"[^\w]+", "_", cleaned, flags=re.UNICODE)  # Thay k√Ω t·ª± ƒë·∫∑c bi·ªát b·∫±ng d·∫•u g·∫°ch d∆∞·ªõi.
    cleaned = re.sub(r"_+", "_", cleaned)  # Gom c√°c d·∫•u '_' li√™n ti·∫øp v·ªÅ m·ªôt d·∫•u.
    return cleaned.strip("_").lower()  # X√≥a '_' d∆∞ th·ª´a ·ªü bi√™n v√† ƒë·ªïi v·ªÅ ch·ªØ th∆∞·ªùng.


def load_raw_dataframe(source: Path, fallback_csv: Path | None) -> pd.DataFrame:
    """Load dataframe t·ª´ pickle/CSV; n·∫øu ngu·ªìn ch√≠nh kh√¥ng c√≥ s·∫Ω fallback sang CSV."""
    if source.exists():
        if source.suffix.lower() == ".pkl":
            print(f"ƒêang ƒë·ªçc pickle: {source}")
            return pd.read_pickle(source)  # ƒê·ªçc nhanh h∆°n v√† gi·ªØ nguy√™n ki·ªÉu d·ªØ li·ªáu.
        if source.suffix.lower() == ".csv":
            print(f"ƒêang ƒë·ªçc CSV: {source}")
            return pd.read_csv(source, low_memory=False)  # ƒê·ªçc CSV v·ªõi dtype ·ªïn ƒë·ªãnh h∆°n.
        raise ValueError(f"ƒê·ªãnh d·∫°ng ngu·ªìn kh√¥ng h·ªó tr·ª£: {source.suffix}")

    if fallback_csv and fallback_csv.exists():
        print(f"Kh√¥ng th·∫•y {source}, fallback sang CSV: {fallback_csv}")
        return pd.read_csv(fallback_csv, low_memory=False)  # ƒê·ªçc file d·ª± ph√≤ng n·∫øu c√≥.

    raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒë·∫ßu v√†o ·ªü {source} ho·∫∑c {fallback_csv}.")


def convert_numeric(df: pd.DataFrame, skip_cols: Iterable[str]) -> pd.DataFrame:
    """C·ªë g·∫Øng √©p c√°c c·ªôt sang s·ªë (tr·ª´ nh·ªØng c·ªôt b·ªã lo·∫°i tr·ª´)."""
    for col in df.columns:
        if col in skip_cols:
            continue  # Kh√¥ng ƒë·ªông v√†o c√°c c·ªôt c·∫ßn gi·ªØ nguy√™n (v√≠ d·ª• c·ªôt nh√£n).
        if pd.api.types.is_numeric_dtype(df[col]):
            continue  # N·∫øu ƒë√£ l√† s·ªë th√¨ kh√¥ng c·∫ßn x·ª≠ l√Ω th√™m.
        coerced = pd.to_numeric(df[col], errors="coerce")  # Th·ª≠ chuy·ªÉn v·ªÅ ki·ªÉu s·ªë; l·ªói s·∫Ω th√†nh NaN.
        df[col] = coerced  # G√°n l·∫°i c·ªôt ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω.
    return df


def fill_missing_values(df: pd.DataFrame, skip_cols: Iterable[str]) -> pd.DataFrame:
    """ƒêi·ªÅn gi√° tr·ªã thi·∫øu: median cho c·ªôt s·ªë, mode cho c·ªôt ph√¢n lo·∫°i."""
    numeric_cols = df.select_dtypes(include=["number"]).columns.difference(skip_cols)
    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
    # Median ·ªïn ƒë·ªãnh h∆°n mean khi c√≥ ngo·∫°i l·ªá, ph√π h·ª£p ƒë·ªÉ ƒëi·ªÅn thi·∫øu cho d·ªØ li·ªáu s·ªë.

    non_numeric_cols = df.columns.difference(numeric_cols.union(skip_cols))
    for col in non_numeric_cols:
        if df[col].isna().any():
            mode = df[col].mode(dropna=True)  # Mode ƒë·∫°i di·ªán cho gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t.
            if not mode.empty:
                df[col] = df[col].fillna(mode.iloc[0])  # ƒêi·ªÅn thi·∫øu b·∫±ng mode ƒë·∫ßu ti√™n.
            else:
                df[col] = df[col].fillna("")  # N·∫øu kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c mode, ƒë·∫∑t r·ªóng ƒë·ªÉ nh·∫•t qu√°n.
    return df


def encode_label(df: pd.DataFrame, label_col: str) -> tuple[pd.DataFrame, dict[int, str]]:
    """M√£ h√≥a c·ªôt nh√£n sang s·ªë v√† tr·∫£ v·ªÅ mapping ng∆∞·ª£c."""
    if label_col not in df.columns:
        print(f"Kh√¥ng t√¨m th·∫•y c·ªôt nh√£n {label_col}, b·ªè qua b∆∞·ªõc m√£ h√≥a.")
        return df, {}

    label_series = df[label_col].astype(str).str.strip()  # ƒê·ªìng nh·∫•t ki·ªÉu d·ªØ li·ªáu v√† b·ªè kho·∫£ng tr·∫Øng.
    df[label_col] = label_series  # C·∫≠p nh·∫≠t l·∫°i c·ªôt g·ªëc sau khi l√†m s·∫°ch.
    codes, uniques = pd.factorize(label_series, sort=True)  # M√£ h√≥a nh√£n th√†nh s·ªë nguy√™n.
    df[f"{label_col}_encoded"] = codes  # Th√™m c·ªôt nh√£n ƒë√£ m√£ h√≥a.
    mapping = {code: label for code, label in enumerate(uniques)}  # B·∫£ng tra c·ª©u ƒë·ªÉ gi·∫£i m√£ l·∫°i.
    return df, mapping


def add_label_group_column(
    df: pd.DataFrame,
    source_col: str,
    group_col: str,
) -> tuple[pd.DataFrame, dict[str, str]]:
    """T·∫°o c·ªôt gom nh√≥m nh√£n theo logic t√πy ch·ªânh."""
    if source_col not in df.columns:
        print(f"Kh√¥ng t√¨m th·∫•y c·ªôt nh√£n {source_col} ƒë·ªÉ t·∫°o nh√≥m.")
        return df, {}

    # ƒê·ªãnh nghƒ©a mapping t·ª´ nh√£n chi ti·∫øt sang nh√≥m t·ªïng qu√°t.
    # Ch·ªâ gi·ªØ l·∫°i: benign, dos, ddos, portscan
    # ƒê√£ b·ªè: Bot, Infiltration, Heartbleed
    group_rules = {
        "benign": "benign",
        "dos hulk": "dos",
        "dos goldeneye": "dos",
        "dos slowloris": "dos",
        "dos slowhttptest": "dos",
        "ddos": "ddos",
        "portscan": "portscan",
        # Bot, Infiltration, Heartbleed ƒë√£ ƒë∆∞·ª£c lo·∫°i b·ªè ·ªü b∆∞·ªõc tr∆∞·ªõc
    }

    default_group = "other"  # Ph√¢n lo·∫°i m·∫∑c ƒë·ªãnh cho nh√£n ch∆∞a ƒë·ªãnh nghƒ©a.
    mapping_report: dict[str, str] = {}

    cleaned_series = df[source_col].astype(str).str.strip()
    # X√¢y d·ª±ng mapping th·ª±c t·∫ø t·ª´ nh√£n g·ªëc -> nh√≥m.
    for original_label in cleaned_series.unique():
        normalized = original_label.lower()
        group_name = group_rules.get(normalized, default_group)
        mapping_report[original_label] = group_name

    group_series = cleaned_series.map(mapping_report)
    df[group_col] = group_series  # G√°n v√†o DataFrame.
    return df, mapping_report


def add_binary_label_column(
    df: pd.DataFrame,
    group_col: str,
    binary_col: str = "label_binary_encoded"
) -> pd.DataFrame:
    """T·∫°o c·ªôt binary label: 0=benign, 1=attack (g·ªôp dos, ddos, portscan)"""
    if group_col not in df.columns:
        print(f"Kh√¥ng t√¨m th·∫•y c·ªôt nh√≥m {group_col} ƒë·ªÉ t·∫°o binary label.")
        return df
    
    if binary_col in df.columns:
        print(f"C·ªôt {binary_col} ƒë√£ t·ªìn t·∫°i, b·ªè qua.")
        return df
    
    def map_to_binary(group_value):
        if pd.isna(group_value):
            return 0
        group_str = str(group_value).lower().strip()
        if group_str == "benign":
            return 0
        else:  # dos, ddos, portscan, other -> attack
            return 1
    
    df[binary_col] = df[group_col].apply(map_to_binary)
    print(f"ƒê√£ t·∫°o c·ªôt binary label: {binary_col}")
    print(f"Binary distribution: {df[binary_col].value_counts().to_dict()}")
    return df


def add_attack_type_label_column(
    df: pd.DataFrame,
    group_col: str,
    attack_type_col: str = "label_attack_type_encoded"
) -> pd.DataFrame:
    """T·∫°o c·ªôt attack type label: 0=dos, 1=ddos, 2=portscan (ch·ªâ cho attack, benign = -1)"""
    if group_col not in df.columns:
        print(f"Kh√¥ng t√¨m th·∫•y c·ªôt nh√≥m {group_col} ƒë·ªÉ t·∫°o attack type label.")
        return df
    
    if attack_type_col in df.columns:
        print(f"C·ªôt {attack_type_col} ƒë√£ t·ªìn t·∫°i, b·ªè qua.")
        return df
    
    def map_to_attack_type(group_value):
        if pd.isna(group_value):
            return -1
        group_str = str(group_value).lower().strip()
        if group_str == "benign":
            return -1  # Kh√¥ng ph·∫£i attack
        elif group_str == "dos":
            return 0
        elif group_str == "ddos":
            return 1
        elif group_str == "portscan":
            return 2
        else:
            return -1  # Unknown
    
    df[attack_type_col] = df[group_col].apply(map_to_attack_type)
    print(f"ƒê√£ t·∫°o c·ªôt attack type label: {attack_type_col}")
    print(f"Attack type distribution: {df[attack_type_col].value_counts().to_dict()}")
    return df


def drop_sparse_columns(
    df: pd.DataFrame, min_ratio: float, skip_cols: Iterable[str]
) -> tuple[pd.DataFrame, list[str]]:
    """Lo·∫°i b·ªè c√°c c·ªôt c√≥ t·ª∑ l·ªá gi√° tr·ªã h·ª£p l·ªá th·∫•p h∆°n ng∆∞·ª°ng cho ph√©p."""
    if min_ratio <= 0:
        return df, []
    dropped: list[str] = []
    for col in df.columns:
        if col in skip_cols:
            continue
        non_null_ratio = df[col].notna().mean()  # T·ª∑ l·ªá ph·∫ßn t·ª≠ kh√¥ng r·ªóng tr√™n to√†n b·ªô c·ªôt.
        if not np.isfinite(non_null_ratio) or non_null_ratio < min_ratio:
            dropped.append(col)
    if dropped:
        df = df.drop(columns=dropped)
    return df, dropped


def drop_constant_columns(
    df: pd.DataFrame, skip_cols: Iterable[str]
) -> tuple[pd.DataFrame, list[str]]:
    """Lo·∫°i b·ªè c√°c c·ªôt ch·ªâ c√≥ m·ªôt gi√° tr·ªã (bao g·ªìm c·∫£ NaN)."""
    dropped: list[str] = []
    for col in df.columns:
        if col in skip_cols:
            continue
        if df[col].nunique(dropna=False) <= 1:  # C·ªôt c√≥ 0-1 gi√° tr·ªã => kh√¥ng h·ªØu d·ª•ng.
            dropped.append(col)
    if dropped:
        df = df.drop(columns=dropped)
    return df, dropped


def clip_outliers_iqr(
    df: pd.DataFrame, numeric_cols: Iterable[str], factor: float
) -> dict[str, dict[str, float]]:
    """Clip ngo·∫°i l·ªá d·ª±a tr√™n kho·∫£ng t·ª© ph√¢n v·ªã (IQR)."""
    if factor <= 0:
        return {}
    clip_info: dict[str, dict[str, float]] = {}
    for col in numeric_cols:
        series = df[col]
        if not np.issubdtype(series.dtype, np.number):
            continue
        q1 = series.quantile(0.25)  # Ph√¢n v·ªã th·ª© 25.
        q3 = series.quantile(0.75)  # Ph√¢n v·ªã th·ª© 75.
        iqr = q3 - q1  # Kho·∫£ng t·ª© ph√¢n v·ªã.
        if not np.isfinite(iqr) or iqr == 0:
            continue
        lower = q1 - factor * iqr  # Ng∆∞·ª°ng d∆∞·ªõi cho clip.
        upper = q3 + factor * iqr  # Ng∆∞·ª°ng tr√™n cho clip.
        df[col] = series.clip(lower=lower, upper=upper)  # Gi·ªõi h·∫°n gi√° tr·ªã ngo√†i kho·∫£ng.
        clip_info[col] = {"lower": float(lower), "upper": float(upper)}
    return clip_info


def one_hot_encode(
    df: pd.DataFrame, categorical_cols: Iterable[str]
) -> tuple[pd.DataFrame, dict[str, list[str]]]:
    """One-hot encoding cho c√°c c·ªôt ph√¢n lo·∫°i, tr·∫£ v·ªÅ mapping danh s√°ch gi√° tr·ªã."""
    categorical_cols = list(categorical_cols)
    if not categorical_cols:
        return df, {}
    category_map: dict[str, list[str]] = {}
    for col in categorical_cols:
        uniques = (
            df[col]
            .astype(str)
            .fillna("__nan__")
            .replace("__nan__", np.nan)
            .dropna()
            .unique()
        )
        # L∆∞u danh s√°ch gi√° tr·ªã ƒë·ªÉ sau n√†y decode/log l·∫°i.
        category_map[col] = sorted(str(val) for val in uniques)
    # pd.get_dummies t·∫°o c√°c c·ªôt nh·ªã ph√¢n t∆∞∆°ng ·ª©ng t·ª´ng gi√° tr·ªã.
    df = pd.get_dummies(df, columns=categorical_cols, drop_first=False, dummy_na=False)
    return df, category_map


def balance_dataset(
    df: pd.DataFrame,  # B·∫£ng d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch.
    label_col: str,  # T√™n c·ªôt nh√£n s·ª≠ d·ª•ng ƒë·ªÉ c√¢n b·∫±ng.
    method: str,  # Chi·∫øn l∆∞·ª£c c√¢n b·∫±ng ƒë∆∞·ª£c ch·ªçn.
    random_state: int,  # Seed t·∫°o ng·∫´u nhi√™n ƒë·ªÉ t√°i l·∫≠p k·∫øt qu·∫£.
) -> tuple[pd.DataFrame, dict[str, dict[str, int]]]:
    """C√¢n b·∫±ng d·ªØ li·ªáu theo nh√£n b·∫±ng c√°ch oversample/undersample."""
    if method == "none":  # N·∫øu ng∆∞·ªùi d√πng kh√¥ng y√™u c·∫ßu c√¢n b·∫±ng.
        return df, {}  # Tr·∫£ v·ªÅ d·ªØ li·ªáu g·ªëc v√† metadata r·ªóng.
    if label_col not in df.columns:  # B·∫£o v·ªá khi c·ªôt nh√£n kh√¥ng t·ªìn t·∫°i.
        print(f"Kh√¥ng t√¨m th·∫•y c·ªôt nh√£n {label_col} ƒë·ªÉ c√¢n b·∫±ng, b·ªè qua.")  # Th√¥ng b√°o cho ng∆∞·ªùi d√πng.
        return df, {}  # Gi·ªØ nguy√™n d·ªØ li·ªáu.
    counts_before = df[label_col].value_counts().to_dict()  # Ghi nh·∫≠n ph√¢n b·ªë ban ƒë·∫ßu ƒë·ªÉ b√°o c√°o.
    rng = np.random.default_rng(random_state)  # T·∫°o generator ng·∫´u nhi√™n d√πng chung trong b∆∞·ªõc oversample.
    target_size = (  # Quy ƒë·ªãnh k√≠ch th∆∞·ªõc m·ª•c ti√™u cho m·ªói l·ªõp.
        max(counts_before.values()) if method == "oversample" else min(counts_before.values())
    )
    balanced_frames: list[pd.DataFrame] = []  # Danh s√°ch ch·ª©a t·ª´ng ph·∫ßn d·ªØ li·ªáu sau x·ª≠ l√Ω theo l·ªõp.
    for label_value, count in counts_before.items():  # L·∫∑p qua t·ª´ng nh√£n v√† s·ªë l∆∞·ª£ng hi·ªán t·∫°i.
        subset = df[df[label_col] == label_value]  # Ch·∫Øt l·ªçc d·ªØ li·ªáu thu·ªôc nh√£n hi·ªán t·∫°i.
        if method == "oversample" and count < target_size:  # Tr∆∞·ªùng h·ª£p l·ªõp nh·ªè c·∫ßn nh√¢n b·∫£n.
            need = target_size - count  # S·ªë m·∫´u c·∫ßn b·ªï sung ƒë·ªÉ ƒë·∫°t m·ª©c m·ª•c ti√™u.
            extra_index = rng.choice(  # Ch·ªçn ng·∫´u nhi√™n ch·ªâ s·ªë (c√≥ l·∫∑p) t·ª´ l·ªõp hi·ªán t·∫°i.
                subset.index.to_numpy(), size=need, replace=True
            )
            subset = pd.concat([subset, df.loc[extra_index]], ignore_index=False)  # Gh√©p th√™m c√°c b·∫£n sao v√†o l·ªõp.
        if method == "undersample" and count > target_size:  # Tr∆∞·ªùng h·ª£p l·ªõp l·ªõn c·∫ßn c·∫Øt gi·∫£m.
            subset = subset.sample(  # L·∫•y ng·∫´u nhi√™n target_size m·∫´u m√† kh√¥ng thay th·∫ø.
                n=target_size, random_state=random_state, replace=False
            )
        balanced_frames.append(subset)  # L∆∞u ph·∫ßn d·ªØ li·ªáu ƒë√£ ƒëi·ªÅu ch·ªânh v√†o danh s√°ch.
    balanced_df = pd.concat(balanced_frames, ignore_index=True)  # G·ªôp t·∫•t c·∫£ l·ªõp l·∫°i th√†nh DataFrame m·ªõi.
    balanced_df = balanced_df.sample(  # X√°o tr·ªôn t·ªïng th·ªÉ ƒë·ªÉ tr√°nh nh√≥m l·ªõp theo block.
        frac=1, random_state=random_state
    ).reset_index(drop=True)
    counts_after = balanced_df[label_col].value_counts().to_dict()  # Th·ªëng k√™ ph√¢n b·ªë sau c√¢n b·∫±ng.
    print(f"ƒê√£ c√¢n b·∫±ng d·ªØ li·ªáu b·∫±ng ph∆∞∆°ng ph√°p {method}.")  # Th√¥ng b√°o ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng.
    print(f"Ph√¢n b·ªë tr∆∞·ªõc: {counts_before}")  # Hi·ªÉn th·ªã ph√¢n b·ªë ban ƒë·∫ßu.
    print(f"Ph√¢n b·ªë sau  : {counts_after}")  # Hi·ªÉn th·ªã ph√¢n b·ªë sau khi c√¢n b·∫±ng.
    return balanced_df, {"before": counts_before, "after": counts_after}  # Tr·∫£ v·ªÅ d·ªØ li·ªáu m·ªõi v√† metadata.


def scale_numeric_features(
    df: pd.DataFrame, numeric_cols: Iterable[str], method: str
) -> tuple[pd.DataFrame, dict[str, dict[str, float]]]:
    """Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë theo ph∆∞∆°ng ph√°p l·ª±a ch·ªçn."""
    stats: dict[str, dict[str, float]] = {}
    if method == "none":
        return df, stats
    numeric_cols = [col for col in numeric_cols if col in df.columns]
    for col in numeric_cols:
        series = df[col]
        if not np.issubdtype(series.dtype, np.number):
            continue
        if method == "standard":
            mean = float(series.mean())  # Gi√° tr·ªã trung b√¨nh ƒë·ªÉ chu·∫©n h√≥a z-score.
            std = float(series.std(ddof=0))  # ƒê·ªô l·ªách chu·∫©n (population).
            if std == 0 or not np.isfinite(std):
                continue
            df[col] = (series - mean) / std  # (x - mean) / std.
            stats[col] = {"mean": mean, "std": std}
        elif method == "minmax":
            min_val = float(series.min())  # Gi√° tr·ªã nh·ªè nh·∫•t d√πng ƒë·ªÉ scale.
            max_val = float(series.max())  # Gi√° tr·ªã l·ªõn nh·∫•t d√πng ƒë·ªÉ scale.
            if max_val == min_val or not np.isfinite(max_val) or not np.isfinite(min_val):
                continue
            df[col] = (series - min_val) / (max_val - min_val)  # ƒê∆∞a v·ªÅ kho·∫£ng [0, 1].
            stats[col] = {"min": min_val, "max": max_val}
        else:
            raise ValueError(f"Ph∆∞∆°ng ph√°p scale kh√¥ng h·ªó tr·ª£: {method}")
    return df, stats


def print_summary(df: pd.DataFrame, label_col: str) -> None:
    """In th·ªëng k√™ t·ªïng quan sau ti·ªÅn x·ª≠ l√Ω."""
    print("\n===== Th·ªëng k√™ d·ªØ li·ªáu sau ti·ªÅn x·ª≠ l√Ω =====")
    print(f"K√≠ch th∆∞·ªõc: {df.shape[0]} d√≤ng x {df.shape[1]} c·ªôt")
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    categorical_cols = df.select_dtypes(exclude=["number"]).columns.tolist()
    print(f"S·ªë c·ªôt s·ªë: {len(numeric_cols)} | S·ªë c·ªôt ph√¢n lo·∫°i: {len(categorical_cols)}")
    if label_col in df.columns:
        print("Ph√¢n b·ªë nh√£n:")
        print(df[label_col].value_counts(dropna=False).to_string())
    print("===========================================\n")


def resolve_path(path: Path | None) -> Path | None:
    """Chuy·ªÉn ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi (so v·ªõi project root) sang tuy·ªát ƒë·ªëi."""
    if path is None:
        return None
    expanded = path.expanduser()  # Thay th·∫ø k√Ω t·ª± '~' n·∫øu c√≥.
    if expanded.is_absolute():
        return expanded  # N·∫øu ƒë√£ l√† ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi th√¨ d√πng lu√¥n.
    return BASE_DIR / expanded


def main() -> None:
    args = parse_args()

    # N·∫øu model_type = "both", ch·∫°y cho c·∫£ hai lo·∫°i model
    if args.model_type == "both":
        print("=" * 80)
        print("üöÄ CH·∫†Y PREPROCESSING CHO C·∫¢ HAI LO·∫†I MODEL")
        print("=" * 80)

        # L∆∞u args g·ªëc ƒë·ªÉ restore
        original_model_type = args.model_type
        original_output = args.output
        original_metadata_output = args.metadata_output

        # Ch·∫°y cho Random Forest
        print("\n" + "="*60)
        print("üîç X·ª¨ L√ù CHO RANDOM FOREST")
        print("="*60)
        args.model_type = "random_forest"
        args.output = Path(str(original_output).replace('.pkl', '_rf.pkl'))
        if original_metadata_output:
            args.metadata_output = Path(str(original_metadata_output).replace('.json', '_rf.json'))
        run_preprocessing_for_model(args)

        # Ch·∫°y cho CNN+LSTM
        print("\n" + "="*60)
        print("üß† X·ª¨ L√ù CHO CNN+LSTM")
        print("="*60)
        args.model_type = "cnn_lstm"
        args.output = Path(str(original_output).replace('.pkl', '_cnn.pkl'))
        if original_metadata_output:
            args.metadata_output = Path(str(original_metadata_output).replace('.json', '_cnn.json'))
        run_preprocessing_for_model(args)

        print("\n" + "="*80)
        print("‚úÖ HO√ÄN TH√ÄNH PREPROCESSING CHO C·∫¢ HAI MODEL")
        print("="*80)
        print(f"üìÑ Random Forest output: {args.output}")
        print(f"üìÑ CNN+LSTM output: {args.output}")
        return

    # Ch·∫°y b√¨nh th∆∞·ªùng cho m·ªôt model
    run_preprocessing_for_model(args)


def run_preprocessing_for_model(args) -> None:
    """Ch·∫°y preprocessing cho m·ªôt lo·∫°i model c·ª• th·ªÉ"""
    print(f"üîß Preprocessing cho model type: {args.model_type}")

    source_path = resolve_path(args.source)  # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu ƒë·∫ßu v√†o ƒë√£ chu·∫©n h√≥a.
    fallback_csv = resolve_path(args.fallback_csv)
    output_path = resolve_path(args.output)  # ƒê∆∞·ªùng d·∫´n l∆∞u pickle sau khi x·ª≠ l√Ω.
    output_csv = resolve_path(args.output_csv) if args.output_csv else None
    metadata_output = resolve_path(args.metadata_output) if args.metadata_output else None

    # 1. ƒê·ªçc d·ªØ li·ªáu th√¥ t·ª´ pickle (∆∞u ti√™n) ho·∫∑c CSV n·∫øu pickle ch∆∞a s·∫µn.
    df = load_raw_dataframe(source_path, fallback_csv)
    print(f"Dataset g·ªëc: {df.shape[0]} rows x {df.shape[1]} columns")

    # 1.1. Lo·∫°i b·ªè c√°c nh√£n kh√¥ng c·∫ßn thi·∫øt: Bot, Infiltration, Heartbleed
    labels_to_remove = ['Bot', 'Infiltration', 'Heartbleed', 'bot', 'infiltration', 'heartbleed']
    before_remove = len(df)
    
    # T√¨m c·ªôt label th·ª±c t·∫ø (c√≥ th·ªÉ c√≥ kho·∫£ng tr·∫Øng ho·∫∑c t√™n kh√°c)
    actual_label_col = None
    label_col_normalized = args.label_column.strip().lower()
    for col in df.columns:
        if col.strip().lower() == label_col_normalized or 'label' in col.lower():
            actual_label_col = col
            break
    
    if actual_label_col:
        df = df[~df[actual_label_col].astype(str).str.strip().str.lower().isin([l.lower() for l in labels_to_remove])]
        removed_count = before_remove - len(df)
        if removed_count > 0:
            print(f"ƒê√£ lo·∫°i b·ªè {removed_count} m·∫´u v·ªõi nh√£n: {', '.join(labels_to_remove)}")
            print(f"Dataset sau khi lo·∫°i b·ªè: {df.shape[0]} rows x {df.shape[1]} columns")

    # 2. Chu·∫©n h√≥a t√™n c·ªôt ƒë·ªÉ d·ªÖ thao t√°c ·ªü c√°c b∆∞·ªõc sau.
    # Chu·∫©n h√≥a to√†n b·ªô t√™n c·ªôt ƒë·ªÉ ƒë·∫£m b·∫£o c√°c b∆∞·ªõc x·ª≠ l√Ω sau kh√¥ng b·ªã l·ªói v√¨ k√Ω t·ª± l·∫°.
    df.columns = [normalize_column(col) for col in df.columns]
    # T√™n c·ªôt nh√£n c≈©ng ph·∫£i chu·∫©n h√≥a gi·ªëng d·ªØ li·ªáu ƒë·ªÉ c√≥ th·ªÉ tra c·ª©u ch√≠nh x√°c.
    label_col = normalize_column(args.label_column)

    # 3. Thay th·∫ø v√¥ h·∫°n b·∫±ng NaN ƒë·ªÉ c√≥ th·ªÉ x·ª≠ l√Ω thi·∫øu nh·∫•t qu√°n.
    df = df.replace([np.inf, -np.inf], np.nan)  # NaN h√≥a gi√° tr·ªã v√¥ h·∫°n ƒë·ªÉ x·ª≠ l√Ω thi·∫øu th·ªëng nh·∫•t.
    # 4. √âp c√°c c·ªôt c√≥ th·ªÉ sang ki·ªÉu s·ªë (tr·ª´ c·ªôt nh√£n).
    df = convert_numeric(df, skip_cols={label_col})
    # 5. ƒêi·ªÅn gi√° tr·ªã thi·∫øu b·∫±ng th·ªëng k√™ ph√π h·ª£p.
    df = fill_missing_values(df, skip_cols={label_col})

    # 5.1. Lo·∫°i b·ªè d√≤ng thi·∫øu nh√£n ƒë·ªÉ tr√°nh l·ªói khi m√£ h√≥a/c√¢n b·∫±ng.
    missing_labels = df[label_col].isna().sum()
    if missing_labels:
        df = df[df[label_col].notna()].reset_index(drop=True)
        print(f"ƒê√£ lo·∫°i b·ªè {missing_labels} d√≤ng thi·∫øu nh√£n ({label_col}).")

    # 6. M√£ h√≥a nh√£n (n·∫øu t·ªìn t·∫°i) v√† chu·∫©n b·ªã mapping ƒë·ªÉ b√°o c√°o.
    df, label_mapping = encode_label(df, label_col)

    label_group_col: str | None = None  # T√™n c·ªôt nh√≥m nh√£n n·∫øu ƒë∆∞·ª£c t·∫°o.
    label_group_mapping: dict[str, str] = {}  # Mapping nh√£n chi ti·∫øt -> nh√≥m.
    label_group_encoded_mapping: dict[int, str] = {}  # Mapping m√£ nh√≥m -> t√™n nh√≥m.
    if args.create_label_group:
        # Chu·∫©n h√≥a t√™n c·ªôt nh√≥m ƒë·ªÉ ƒë·ªìng b·ªô v·ªõi c√°c c·ªôt kh√°c.
        label_group_col = normalize_column(args.label_group_column)
        df, label_group_mapping = add_label_group_column(df, label_col, label_group_col)
        df, label_group_encoded_mapping = encode_label(df, label_group_col)
        
        # T·∫°o binary label v√† attack type label cho Level 1 v√† Level 2
        # Kh√¥ng c·∫ßn encode v√¨ ch√∫ng ƒë√£ l√† s·ªë (0/1 ho·∫∑c 0/1/2/-1)
        binary_col = normalize_column("label_binary_encoded")
        attack_type_col = normalize_column("label_attack_type_encoded")
        df = add_binary_label_column(df, label_group_col, binary_col)
        df = add_attack_type_label_column(df, label_group_col, attack_type_col)
        # Kh√¥ng encode binary v√† attack_type v√¨ ch√∫ng ƒë√£ l√† s·ªë r·ªìi

    # T·∫≠p c√°c c·ªôt c·∫ßn b·ªè qua (gi·ªØ nguy√™n) ·ªü nh·ªØng b∆∞·ªõc x·ª≠ l√Ω kh√°c.
    skip_cols = {label_col, f"{label_col}_encoded"}
    if label_group_col:
        skip_cols.update({label_group_col, f"{label_group_col}_encoded"})
        # Th√™m binary v√† attack type columns v√†o skip_cols (kh√¥ng c√≥ _encoded v√¨ ch√∫ng ƒë√£ l√† s·ªë)
        binary_col = normalize_column("label_binary_encoded")
        attack_type_col = normalize_column("label_attack_type_encoded")
        if binary_col in df.columns:
            skip_cols.add(binary_col)
        if attack_type_col in df.columns:
            skip_cols.add(attack_type_col)

    # 7. Lo·∫°i b·ªè c√°c c·ªôt c√≥ qu√° nhi·ªÅu gi√° tr·ªã thi·∫øu (quality control).
    df, sparse_dropped = drop_sparse_columns(df, args.min_non_null_ratio, skip_cols)
    if sparse_dropped:
        print(f"ƒê√£ lo·∫°i b·ªè {len(sparse_dropped)} c·ªôt thi·∫øu d·ªØ li·ªáu: {sparse_dropped}")

    # 8. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng mang th√¥ng tin (ch·ªâ c√≥ m·ªôt gi√° tr·ªã).
    constant_dropped: list[str] = []
    if args.drop_constant_columns:
        df, constant_dropped = drop_constant_columns(df, skip_cols)
        if constant_dropped:
            print(f"ƒê√£ lo·∫°i b·ªè {len(constant_dropped)} c·ªôt constant: {constant_dropped}")

    # 9. X·ª≠ l√Ω d·ªØ li·ªáu tr√πng l·∫∑p ƒë·ªÉ tr√°nh bias khi training.
    duplicate_count = 0
    if args.drop_duplicates:
        before = len(df)
        df = df.drop_duplicates().reset_index(drop=True)
        duplicate_count = before - len(df)
        if duplicate_count:
            print(f"ƒê√£ lo·∫°i b·ªè {duplicate_count} d√≤ng tr√πng l·∫∑p.")

    # 10. Chu·∫©n h√≥a ngo·∫°i l·ªá (outlier) b·∫±ng c√°ch clip theo kho·∫£ng IQR.
    clip_stats: dict[str, dict[str, float]] = {}
    numeric_cols = df.select_dtypes(include=["number"]).columns.difference(skip_cols)
    if args.outlier_method == "iqr_clip":
        clip_stats = clip_outliers_iqr(df, numeric_cols, args.iqr_factor)
        if clip_stats:
            print(f"ƒê√£ clip ngo·∫°i l·ªá cho {len(clip_stats)} c·ªôt theo IQR (factor={args.iqr_factor}).")

    # 11. M√£ h√≥a one-hot ƒë·ªÉ chuy·ªÉn bi·∫øn ph√¢n lo·∫°i sang d·∫°ng s·ªë nh·ªã ph√¢n.
    category_mapping: dict[str, list[str]] = {}

    # Logic one-hot encoding ph·ª• thu·ªôc v√†o model type
    if args.model_type == "random_forest":
        # Random Forest: Th∆∞·ªùng c·∫ßn one-hot encoding
        if not args.one_hot:
            print("‚úì Auto-enable --one-hot cho Random Forest (c·∫ßn one-hot cho categorical features)")
            args.one_hot = True

    elif args.model_type == "cnn_lstm":
        # CNN+LSTM: Kh√¥ng c·∫ßn one-hot, c√≥ th·ªÉ x·ª≠ l√Ω categorical d∆∞·ªõi d·∫°ng s·ªë nguy√™n
        if args.one_hot:
            print("‚ö†Ô∏è  CNN+LSTM kh√¥ng c·∫ßn one-hot encoding (c√≥ th·ªÉ x·ª≠ l√Ω categorical d∆∞·ªõi d·∫°ng s·ªë nguy√™n)")
            print("‚úì T·ª± ƒë·ªông b·ªè qua --one-hot cho CNN+LSTM")
            args.one_hot = False
        print("‚úì CNN+LSTM s·∫Ω x·ª≠ l√Ω categorical features d∆∞·ªõi d·∫°ng s·ªë nguy√™n (kh√¥ng one-hot)")

    if args.one_hot:
        categorical_cols = df.select_dtypes(include=["object", "category"]).columns.difference(skip_cols)
        df, category_mapping = one_hot_encode(df, categorical_cols)
        if category_mapping:
            print(f"‚úì ƒê√£ one-hot {len(category_mapping)} c·ªôt ph√¢n lo·∫°i cho {args.model_type}.")

    # 12. C√¢n b·∫±ng d·ªØ li·ªáu n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu (oversample/undersample).
    balance_stats: dict[str, dict[str, int]] = {}  # L∆∞u metadata v·ªÅ c√¢n b·∫±ng d·ªØ li·ªáu.
    if args.balance_method != "none":  # Ch·ªâ ch·∫°y khi ng∆∞·ªùi d√πng b·∫≠t t√πy ch·ªçn.
        target_col = f"{label_col}_encoded" if f"{label_col}_encoded" in df.columns else label_col  # ∆Øu ti√™n d√πng nh√£n ƒë√£ m√£ h√≥a.
        df, balance_stats = balance_dataset(  # Th·ª±c hi·ªán c√¢n b·∫±ng theo c·∫•u h√¨nh CLI.
            df,
            target_col,
            args.balance_method,
            args.balance_random_state,
        )

    # 12. Chu·∫©n h√≥a gi√° tr·ªã s·ªë v·ªÅ c√πng thang ƒëo (Standard/MinMax) ƒë·ªÉ m√¥ h√¨nh d·ªÖ h·ªçc.
    scaling_stats: dict[str, dict[str, float]] = {}

    # Logic scale ph·ª• thu·ªôc v√†o model type
    if args.model_type == "random_forest":
        # Random Forest: Scale ·ªü preprocessing, model s·∫Ω kh√¥ng scale l·∫°i
        if args.scale_method == "none":
            # Auto set scale_method cho Random Forest
            args.scale_method = "standard"
            print("‚úì Auto-set --scale-method=standard cho Random Forest (model kh√¥ng c√≥ internal scaler)")
        numeric_cols = df.select_dtypes(include=["number"]).columns.difference(skip_cols)
        df, scaling_stats = scale_numeric_features(df, numeric_cols, args.scale_method)
        if scaling_stats:
            print(f"‚úì ƒê√£ scale {len(scaling_stats)} c·ªôt theo ph∆∞∆°ng ph√°p {args.scale_method} cho Random Forest.")
            print("‚úì Random Forest s·∫Ω d√πng data ƒë√£ scale n√†y tr·ª±c ti·∫øp.")

    elif args.model_type == "cnn_lstm":
        # CNN+LSTM: Kh√¥ng scale ·ªü preprocessing, ƒë·ªÉ model t·ª± scale
        if args.scale_method != "none":
            print("=" * 80)
            print("‚ö†Ô∏è  C·∫¢NH B√ÅO: DOUBLE SCALING DETECTED CHO CNN+LSTM!")
            print("=" * 80)
            print("‚ö†Ô∏è  CNN+LSTM training pipeline ƒë√£ c√≥ StandardScaler.")
            print("‚ö†Ô∏è  N·∫øu scale ·ªü ƒë√¢y s·∫Ω b·ªã double scaling ‚Üí k·∫øt qu·∫£ prediction sai!")
            print("=" * 80)
            print("‚úì T·ª± ƒë·ªông b·ªè qua scaling cho CNN+LSTM - model s·∫Ω t·ª± scale khi training")
            print("=" * 80)
        else:
            print("‚úì Kh√¥ng scale data trong preprocessing cho CNN+LSTM (ƒë√∫ng - model s·∫Ω t·ª± scale khi training).")
    else:
        # Fallback
        if args.scale_method != "none":
            print("=" * 80)
            print("‚ö†Ô∏è  C·∫¢NH B√ÅO NGHI√äM TR·ªåNG: DOUBLE SCALING DETECTED!")
            print("=" * 80)
            print("‚ö†Ô∏è  Model training pipeline ƒë√£ c√≥ StandardScaler.")
            print("‚ö†Ô∏è  N·∫øu scale ·ªü ƒë√¢y s·∫Ω b·ªã double scaling ‚Üí k·∫øt qu·∫£ prediction sai!")
            print("=" * 80)
            print("‚úì Khuy·∫øn ngh·ªã: S·ª≠ d·ª•ng --scale-method none ho·∫∑c --model-type ƒë·ªÉ ch·ªâ ƒë·ªãnh lo·∫°i model")
            print("=" * 80)
            # V·∫´n ti·∫øp t·ª•c scale n·∫øu user y√™u c·∫ßu, nh∆∞ng c·∫£nh b√°o r√µ r√†ng
            numeric_cols = df.select_dtypes(include=["number"]).columns.difference(skip_cols)
            df, scaling_stats = scale_numeric_features(df, numeric_cols, args.scale_method)
            if scaling_stats:
                print(f"‚ö†Ô∏è  ƒê√£ scale {len(scaling_stats)} c·ªôt theo ph∆∞∆°ng ph√°p {args.scale_method}.")
                print("‚ö†Ô∏è  Model s·∫Ω scale l·∫°i data n√†y ‚Üí DOUBLE SCALING!")
                print("‚ö†Ô∏è  K·∫øt qu·∫£ prediction s·∫Ω SAI! Vui l√≤ng retrain v·ªõi --scale-method none!")
        else:
            print("‚úì Kh√¥ng scale data trong preprocessing (ƒë√∫ng - model s·∫Ω t·ª± scale khi training).")

    if args.summary:
        print_summary(df, label_col)
        if label_group_col and label_group_col in df.columns:
            print("Ph√¢n b·ªë nh√£n nh√≥m:")
            print(df[label_group_col].value_counts(dropna=False).to_string())
            print("===========================================\n")

    # 13. L∆∞u k·∫øt qu·∫£ ti·ªÅn x·ª≠ l√Ω ra pickle, v√† CSV n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu.
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_pickle(output_path)
    print(f"ƒê√£ l∆∞u pickle s·∫°ch t·∫°i: {output_path}")

    if output_csv:
        output_csv.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(output_csv, index=False)
        print(f"ƒê√£ l∆∞u CSV s·∫°ch t·∫°i: {output_csv}")

    if label_mapping:
        print("B·∫£ng mapping nh√£n:")
        for code, label in label_mapping.items():
            print(f"  {code}: {label}")

    metadata: dict[str, object] = {
        "rows": int(df.shape[0]),
        "columns": int(df.shape[1]),
        "label_column": label_col,
        "label_mapping": label_mapping,
        "label_group_column": label_group_col,
        "label_group_mapping": label_group_mapping,
        "model_type": args.model_type,
    }
    if label_group_encoded_mapping:
        metadata["label_group_encoded_mapping"] = label_group_encoded_mapping
    metadata.update({
        "sparse_columns_dropped": sparse_dropped,
        "constant_columns_dropped": constant_dropped,
        "duplicates_removed": int(duplicate_count),
        "clip_stats": clip_stats,
        "scaling_method": args.scale_method,
        "scaling_stats": scaling_stats,
        "one_hot_mapping": category_mapping,
        "balance_method": args.balance_method,  # Ghi nh·∫≠n ph∆∞∆°ng ph√°p c√¢n b·∫±ng ƒë∆∞·ª£c s·ª≠ d·ª•ng.
        "balance_stats": balance_stats,  # L∆∞u ph√¢n b·ªë tr∆∞·ªõc/sau cho m·ª•c ƒë√≠ch audit.
    })
    if metadata_output:
        metadata_output.parent.mkdir(parents=True, exist_ok=True)
        with metadata_output.open("w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        print(f"ƒê√£ l∆∞u metadata ti·ªÅn x·ª≠ l√Ω t·∫°i: {metadata_output}")

    print("Ho√†n t·∫•t ti·ªÅn x·ª≠ l√Ω.")


if __name__ == "__main__":
    main()


